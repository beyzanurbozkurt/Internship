{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fwY7bWmE1J1ku19jcDf6SnRH4cRdIsph","timestamp":1735927001421},{"file_id":"1yvhzTmLnQnQMbpk1sLWg-rMmP7h_vETp","timestamp":1735912451525}],"authorship_tag":"ABX9TyPdF4NgCAUUc8ToEC3sm1NR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Cmeft3V9EmQk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735927153284,"user_tz":-180,"elapsed":102862,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}},"outputId":"77c1317f-215c-426d-922c-a9ff5d7fe13b","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torchvision import transforms, models\n","import pandas as pd\n","from PIL import Image\n","from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","from torchvision.models import vit_b_16, ViT_B_16_Weights\n","from timm import create_model\n","\n","\n","\n","\n","# Define the dataset class\n","class AUDataset(Dataset):\n","    def __init__(self, csv_file, image_dir, transform=None):\n","        self.data = pd.read_csv(csv_file)\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.data['fname'] = self.data['fname'].astype(str)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.image_dir, self.data.iloc[idx]['fname'])\n","        image = Image.open(img_name).convert(\"RGB\")\n","        labels = torch.tensor(self.data.iloc[idx, 1:].values.astype('float32'))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, labels\n","\n","# Define the improved DeiT model\n","class AUModel(nn.Module):\n","    def __init__(self, num_aus):\n","        super(AUModel, self).__init__()\n","\n","        self.base_model = create_model('deit_small_patch16_224', pretrained=True)\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.base_model.head = nn.Linear(self.base_model.head.in_features, num_aus)\n","\n","    def forward(self, x):\n","        x = self.base_model(x)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","# Training function\n","def train_model(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs=10):\n","    best_val_f1 = 0.0\n","    best_model_state = None\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        train_loss, train_preds, train_labels = 0, [], []\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_preds.extend(torch.sigmoid(outputs).cpu().detach().numpy() > 0.5)\n","            train_labels.extend(labels.cpu().numpy())\n","\n","        train_f1 = f1_score(train_labels, train_preds, average='macro')\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss, val_preds, val_labels = 0, [], []\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","                val_preds.extend(torch.sigmoid(outputs).cpu().numpy() > 0.5)\n","                val_labels.extend(labels.cpu().numpy())\n","\n","        val_f1 = f1_score(val_labels, val_preds, average='macro')\n","\n","        # Test phase\n","        test_preds, test_labels = [], []\n","        with torch.no_grad():\n","            for inputs, labels in test_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                test_preds.extend(torch.sigmoid(outputs).cpu().numpy() > 0.5)\n","                test_labels.extend(labels.cpu().numpy())\n","\n","        test_f1 = f1_score(test_labels, test_preds, average='macro')\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}\")\n","\n","        # Save best model based on validation F1 score\n","        if val_f1 > best_val_f1:\n","            best_val_f1 = val_f1\n","            best_model_state = model.state_dict()\n","\n","    model.load_state_dict(best_model_state)\n","    return model\n","\n","# Confusion matrix display function\n","def plot_confusion_matrix(true_labels, predictions, classes):\n","    cm = confusion_matrix(true_labels, predictions)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n","    disp.plot(cmap=plt.cm.Blues)\n","    plt.show()\n","\n","# File paths\n","train_csv_files = [\n","    'drive/My Drive/tez/train_new/CAFE_AU_labels_strat_fold_0.csv',\n","    'drive/My Drive/tez/train_new/CAFE_AU_labels_strat_fold_1.csv',\n","    'drive/My Drive/tez/train_new/CAFE_AU_labels_strat_fold_2.csv',\n","]\n","test_csv_files = [\n","    'drive/My Drive/tez/test_new/CAFE_AU_labels_strat_fold_0.csv',\n","    'drive/My Drive/tez/test_new/CAFE_AU_labels_strat_fold_1.csv',\n","    'drive/My Drive/tez/test_new/CAFE_AU_labels_strat_fold_2.csv',\n","]\n","image_dir = 'drive/My Drive/tez/CroppedFromPhotos'\n","\n","# Read Action Unit columns\n","au_columns = pd.read_csv(train_csv_files[0]).columns[1:]\n","num_aus = len(au_columns)\n","\n","# Data transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Loss function and device\n","criterion = nn.BCEWithLogitsLoss()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model\n","best_model = None\n","all_predictions = []\n","\n","# Iterate through each train-test pair\n","for train_csv, test_csv in zip(train_csv_files, test_csv_files):\n","    # Create datasets\n","    full_train_dataset = AUDataset(train_csv, image_dir, transform=transform)\n","    test_dataset = AUDataset(test_csv, image_dir, transform=transform)\n","\n","    # Split train dataset into train and validation sets\n","    train_size = int(0.8 * len(full_train_dataset))\n","    val_size = len(full_train_dataset) - train_size\n","    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","    # Initialize the Vision Transformer model\n","    model = AUModel(num_aus).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","    # Train the model\n","    model = train_model(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs=10)\n","\n","    # Evaluate on the test dataset\n","    test_preds, test_labels = [], []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            outputs = torch.sigmoid(model(inputs))\n","            test_preds.extend(outputs.cpu().numpy() > 0.5)\n","            test_labels.extend(labels.cpu().numpy())\n","\n","    # Save predictions\n","    fold_predictions = pd.DataFrame(test_preds, columns=au_columns)\n","    fold_predictions['fname'] = test_dataset.data['fname'].iloc[:len(test_preds)].values\n","    all_predictions.append(fold_predictions)\n","\n","\n","# Combine and save all predictions\n","final_predictions = pd.concat(all_predictions, ignore_index=True)\n","final_predictions.to_csv('all_test_predictions_with_val.csv', index=False)\n","\n","print(\"Training complete. Predictions saved to all_test_predictions_with_val.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1735929218980,"user_tz":-180,"elapsed":272400,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}},"outputId":"19143905-1901-4b85-c20a-a3c2faa42e68","id":"jS0-6FCzsTk6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train F1: 0.0519, Val F1: 0.1242, Test F1: 0.0859\n","Epoch 2/10, Train F1: 0.1611, Val F1: 0.2162, Test F1: 0.2421\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-52d06bcb24c3>\u001b[0m in \u001b[0;36m<cell line: 151>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# Evaluate on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-52d06bcb24c3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","\n","# Confusion matrix display function for subplot\n","def plot_confusion_matrix_subplot(true_labels, predictions, classes, ax):\n","    cm = confusion_matrix(true_labels, predictions)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n","    disp.plot(cmap=plt.cm.Blues, ax=ax)\n","    ax.set_title(f'Confusion Matrix')\n","\n","# Test phase to collect predictions and true labels\n","test_preds, test_labels = [], []\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.to(device)\n","        outputs = torch.sigmoid(model(inputs))  # Sigmoid output for multi-label classification\n","        preds = (outputs > 0.5).cpu().numpy()  # Apply threshold of 0.5 for binary classification\n","\n","        # Collect predictions and true labels\n","        test_preds.extend(preds)\n","        test_labels.extend(labels.cpu().numpy())\n","\n","# Convert to numpy arrays for sklearn compatibility\n","test_preds = np.array(test_preds)\n","test_labels = np.array(test_labels)\n","\n","# Set up subplot grid (4 columns)\n","num_classes = test_preds.shape[1]\n","ncols = 4\n","nrows = (num_classes + ncols - 1) // ncols  # Calculate number of rows needed\n","\n","fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 4))\n","axes = axes.flatten()  # Flatten the axes array for easier indexing\n","\n","# Plot confusion matrices in subplots\n","for i in range(num_classes):\n","    ax = axes[i]\n","    plot_confusion_matrix_subplot(test_labels[:, i], test_preds[:, i], classes=[0, 1], ax=ax)\n","\n","    # Adjust titles for each subplot\n","    ax.set_title(f'Class: {au_columns[i]}')\n","\n","# Remove any unused subplots\n","for j in range(num_classes, len(axes)):\n","    axes[j].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"XYMcGAtAEmm_","executionInfo":{"status":"aborted","timestamp":1735928461834,"user_tz":-180,"elapsed":2,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Compute metrics for each Action Unit (AU)\n","results = []  # To store metrics for each AU\n","\n","for i in range(test_preds.shape[1]):  # Loop through each AU\n","    true_positive = np.sum((test_labels[:, i] == 1) & (test_preds[:, i] == 1))  # True positives\n","    false_positive = np.sum((test_labels[:, i] == 0) & (test_preds[:, i] == 1))  # False positives\n","    false_negative = np.sum((test_labels[:, i] == 1) & (test_preds[:, i] == 0))  # False negatives\n","    true_negative = np.sum((test_labels[:, i] == 0) & (test_preds[:, i] == 0))  # True negatives\n","\n","    total_samples = true_positive + false_positive + false_negative + true_negative  # Total samples for the AU\n","\n","    results.append({\n","        \"AU\": au_columns[i],\n","        \"Total Samples\": total_samples,\n","        \"Correctly Predicted\": true_positive + true_negative,\n","        \"False Positives\": false_positive,\n","        \"False Negatives\": false_negative,\n","    })\n","\n","# Convert results to a DataFrame for better visualization\n","results_df = pd.DataFrame(results)\n","\n","# Print the results as a table\n","print(results_df)\n","\n","# Optionally, save the results to a CSV file\n","results_df.to_csv(\"AU_metrics_total.csv\", index=False)\n"],"metadata":{"id":"XMLIR1ejbu2i","executionInfo":{"status":"aborted","timestamp":1735928461834,"user_tz":-180,"elapsed":2,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Train CSV dosyalarının yollarını listeleyin\n","train_csv_files = [\n","    'drive/My Drive/tez/train_new/CAFE_AU_labels_strat_fold_0.csv',\n","    'drive/My Drive/tez/train_new/CAFE_AU_labels_strat_fold_1.csv',\n","    'drive/My Drive/tez/train_new/CAFE_AU_labels_strat_fold_2.csv',\n","]\n","\n","# Tüm CSV'leri birleştirin\n","train_data = pd.concat([pd.read_csv(csv_file) for csv_file in train_csv_files], ignore_index=True)\n","\n","# Duplike satırları kaldırın\n","train_data_cleaned = train_data.drop_duplicates()\n","\n","# Yeni temizlenmiş veriyi kaydedin\n","train_data_cleaned.to_csv('combined_train_data.csv', index=False)\n","\n","print(f\"Orijinal veri sayısı: {len(train_data)}, Temizlenmiş veri sayısı: {len(train_data_cleaned)}\")\n"],"metadata":{"id":"utnDWbvqEmpT","executionInfo":{"status":"aborted","timestamp":1735928461835,"user_tz":-180,"elapsed":3,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = pd.read_csv(\"all_test_predictions_with_val.csv\")\n","actual = pd.read_csv(\"combined_train_data.csv\")\n","pred = pred.replace({True: 1, False: 0})\n","columns = ['fname'] + [col for col in pred.columns if col != 'fname']\n","pred = pred[columns]\n","pred.to_csv('pred.csv', index=False)"],"metadata":{"id":"-rsLwxVAbo7b","executionInfo":{"status":"aborted","timestamp":1735928461835,"user_tz":-180,"elapsed":3,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = pd.read_csv(\"pred.csv\")\n","common_rows = pd.merge(predictions, actual)\n","print(f\"Common rows: {len(common_rows)}\")"],"metadata":{"id":"yaCZ-FoGbo9j","executionInfo":{"status":"aborted","timestamp":1735928461835,"user_tz":-180,"elapsed":3,"user":{"displayName":"Beyza Nur Keskin","userId":"15346924660563692244"}}},"execution_count":null,"outputs":[]}]}